---
title: "Pop Goes Profanity: Vulgarity Trends in English Chart Hits from 2000 to 2024 - Analysis"
format: html
editor: visual
---

## Introduction

This document contains the analysis of a study of vulgarity in pop music.

## Preparation

Install required packages

```{r eval = F}
install.packages(c("stringr", "dplyr","readxl", "here", "ggplot2", "forcats",
                   "partykit", "tree", "ggparty", "sjPlot", "car", "rms", 
                   "Hmisc", "lme4", "glmulti", "performance"))
```

Activate packages

```{r}
library(stringr)
library(dplyr)
library(readxl)
library(here)
library(ggplot2)
library(forcats)
library(partykit)
library(tree)
library(ggparty)
library(sjPlot)
library(car)
library(rms)
library(Hmisc)
library(lme4)
library(glmulti)
library(performance)
```

## Processing

Load data

```{r}
dat <- readxl::read_excel(here::here("data", "vulgaritypop_master.xlsx"))
# inspect
str(dat)
```

add topic labels

```{r}
dat <- dat |>
  dplyr::mutate(assigned_topic = as.character(assigned_topic),
                assigned_topic = dplyr::case_when(
    assigned_topic == "1" ~ "Clubbing",
    assigned_topic == "2" ~ "Street Culture",
    assigned_topic == "3" ~ "Love",
    assigned_topic == "4" ~ "Heartbreak",
    assigned_topic == "5" ~ "Belonging",
    T ~ assigned_topic)) |>
  dplyr::mutate(assigned_topic = factor(assigned_topic, 
                                        levels = c("Street Culture", "Clubbing", "Heartbreak", "Belonging", "Love")))
# inspect
table(dat$assigned_topic)

```

recode categories of vulgarity use

```{r}
dat <- dat |>
  dplyr::mutate(vulgarity_level = dplyr::case_when(
    is.na(vulgarity_level) ~ "None",
    vulgarity_level == "NA" ~ "None",
    T ~ vulgarity_level)) |>
  dplyr::mutate(vulgarity_level = factor(vulgarity_level, 
                                            levels = c("Heavy", "Moderate", "Light", "None")))
# inspect
table(dat$vulgarity_level)
```

recode nationality

```{r}
dat <- dat |>
  dplyr::mutate(solo_nationality = dplyr::case_when(
    solo_nationality != "American" ~ "NonAmerican",
    T ~ solo_nationality)) |>
  dplyr::mutate(solo_nationality = factor(solo_nationality, 
                                            levels = c("American", "NonAmerican")))
# inspect
table(dat$solo_nationality)
```

```{r}
table(dat$english_classification)
```


```{r}
dat <- dat |>
  # remove non-English
  dplyr::filter(english_classification != "other")
```

```{r}
table(dat$duplicate_marker)
```

overview by year

```{r}
dat |>
  group_by(year_chart) |>
  dplyr::summarise(songs = n(),
                   words = sum(no_words),
                   vulgar_words = sum(total_bad_word_count),
                   vulgar_songs = round(sum(contains_vulgarity)/n()*100, 1),
                   vulgar_rfreq = round(vulgar_words/words*1000, 1),
                   vulgar_by_song = round(vulgar_words/songs, 1))  |>
  dplyr::ungroup()  |>
  dplyr::mutate(year_chart = as.character(year_chart)) -> table01

# add total row at bottom
table01 <- table01 |>
  dplyr::add_row(
    year_chart = "Total",
    songs = sum(table01$songs, na.rm = TRUE),
    words = sum(table01$words, na.rm = TRUE),
    vulgar_words = sum(table01$vulgar_words, na.rm = TRUE),
    vulgar_songs = round(mean(table01$vulgar_songs, na.rm = TRUE), 1),
    vulgar_rfreq = round(mean(table01$vulgar_rfreq, na.rm = TRUE), 1),
    vulgar_by_song = round(mean(table01$vulgar_by_song, na.rm = TRUE), 1))
# save
write.table(table01, here::here("tables/table01.txt"), sep = "\t", row.names = F)
# inspect
table01
```

overview by topic

```{r}
dat |>
  group_by(assigned_topic) |>
  dplyr::summarise(songs = n(),
                   words = sum(no_words),
                   vulgar_words = sum(total_bad_word_count),
                   vulgar_songs = round(sum(contains_vulgarity)/n()*100, 1),
                   vulgar_rfreq = round(vulgar_words/words*1000, 1),
                   vulgar_by_song = round(vulgar_words/songs, 1))  |>
  dplyr::ungroup()  |>
  dplyr::mutate(assigned_topic = as.character(assigned_topic)) -> table02

# add total row at bottom
table02 <- table02 |>
  dplyr::add_row(
    assigned_topic = "Total",
    songs = sum(table02$songs, na.rm = TRUE),
    words = sum(table02$words, na.rm = TRUE),
    vulgar_words = sum(table02$vulgar_words, na.rm = TRUE),
    vulgar_songs = round(mean(table02$vulgar_songs, na.rm = TRUE), 1),
    vulgar_rfreq = round(mean(table02$vulgar_rfreq, na.rm = TRUE), 1),
    vulgar_by_song = round(mean(table02$vulgar_by_song, na.rm = TRUE), 1))
# save
write.table(table01, here::here("tables/table02.txt"), sep = "\t", row.names = F)
# inspect
table02
```



# Visuals

vulgarity over time by genre

```{r}
dat |>
  group_by(year_chart, genre) |>
  summarise(words = sum(no_words, na.rm = TRUE),
            vulgar = sum(total_bad_word_count, na.rm = TRUE),
            .groups = "drop_last") |>
  mutate(freq = vulgar / words * 1000) |>
  filter(!is.na(freq)) |>
  ungroup() |>
  group_by(genre) |>
  mutate(vulgar_genre = sum(vulgar, na.rm = TRUE)) |>
  ungroup() |>
  filter(vulgar_genre != 0) |>
  # Reorder genre factor by total vulgar words (descending)
  mutate(genre = forcats::fct_reorder(genre, vulgar_genre, .desc = TRUE)) |>
  ggplot(aes(x = year_chart, y = freq, group = genre, color = genre, linetype = genre)) +
  geom_line(linewidth = 0.5) +
  theme_bw() +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  scale_color_manual(values = gray.colors(9, start = 0.1, end = 0.7)) +
  labs(y = "Relative frequency of vulgar elements\n(per 1,000 words)",
       x = "Year")
# save
ggsave(here::here("images/Fig1.png"), width = 18, height = 10, units = "cm")
```

genres without vulgarity

```{r}
dat |>
  dplyr::group_by(genre) |>
  dplyr::summarise(vulgar = sum(total_bad_word_count)) |>
  dplyr::arrange(-vulgar) |>
  dplyr::ungroup()  |>
  dplyr::filter(vulgar == 0)
```

vulgarity over time by topic

```{r}
dat |>
  group_by(year_chart, assigned_topic) |>
  summarise(words = sum(no_words, na.rm = TRUE),
            vulgar = sum(total_bad_word_count, na.rm = TRUE),
            .groups = "drop_last") |>
  mutate(freq = vulgar / words * 1000) |>
  filter(!is.na(freq)) |>
  ungroup() |>
  group_by(assigned_topic) |>
  mutate(vulgar_topic = sum(vulgar, na.rm = TRUE)) |>
  ungroup() |>
  filter(vulgar_topic != 0) |>
  # Reorder genre factor by total vulgar words (descending)
  mutate(genre = forcats::fct_reorder(assigned_topic, vulgar_topic, .desc = TRUE)) |>
  ggplot(aes(x = year_chart, y = freq, group = assigned_topic, color = assigned_topic, linetype = assigned_topic)) +
  geom_line(linewidth = 0.5) +
  theme_bw() +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  scale_color_manual(values = gray.colors(9, start = 0.1, end = 0.7)) +
  labs(y = "Relative frequency of vulgar elements\n(per 1,000 words)",
       x = "Year")
# save
ggsave(here::here("images/Fig5.png"), width = 18, height = 10, units = "cm")
```

severity of vulgarity over time

```{r}
dat |>
  group_by(year_chart, vulgarity_level) |>
  summarise(count = n()) |>
  dplyr::ungroup() |>
  group_by(year_chart) |>
  mutate(total = sum(count),
         percent = round(count/total*100, 1)) |>
  filter(!is.na(percent)) |>
  ungroup() |>
  ggplot(aes(x = year_chart, y = percent, group = vulgarity_level, color = vulgarity_level, linetype = vulgarity_level)) +
  geom_line(linewidth = 0.5) +
  theme_bw() +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  scale_color_manual(values = gray.colors(4, start = 0.1, end = 0.7)) +
  labs(y = "Percentage of vulgarity levels",
       x = "Year")
# save
ggsave(here::here("images/Fig6.png"), width = 18, height = 10, units = "cm")
```

overall vulgarity over time

```{r}
dat |>
  dplyr::group_by(year_chart) |>
  dplyr::summarise(words = sum(no_words),
                   vulgar = sum(total_bad_word_count)) |>
  dplyr::mutate(freq = vulgar/words*1000) |>
  dplyr::filter(!is.na(freq)) |>
  dplyr::ungroup()  |>
  ggplot2::ggplot(aes(x = year_chart, y = freq)) +
  geom_line() +
  geom_smooth(color = "black", linewidth = 0.5, linetype = "dotted") +
  theme_bw() +
  labs(y = "Relative frequency of vulgar elements\n(per 1,000 words)",
       x = "Year")
# save
ggsave(here::here("images/Fig0.png"), width = 18, height = 10, units = "cm")
```

percentage of songs with vulgarity over time



percentage of songs with vulgarity

```{r}
dat |>
  dplyr::group_by(year_chart) |>
  dplyr::summarise(songs = n(),
                   vulgar = sum(contains_vulgarity)) |>
  dplyr::mutate(percent = vulgar/songs*100) |>
  dplyr::filter(!is.na(percent)) |>
  dplyr::ungroup()  |>
  ggplot2::ggplot(aes(x = year_chart, y = percent)) +
  geom_line() +
  geom_smooth(color = "black", linewidth = 0.5, linetype = "dotted") +
  theme_bw() +
  labs(y = "Percentage of songs with vulgarity",
       x = "Year")
# save
ggsave(here::here("images/Fig2.png"), width = 18, height = 10, units = "cm")
```


overall vulgarity over time

```{r}
dat |>
  dplyr::filter(!is.na(solo_gender),
                solo_gender != "NA") |>
  dplyr::group_by(year_chart, solo_gender) |>
  dplyr::summarise(words = sum(no_words),
                   vulgar = sum(total_bad_word_count)) |>
  dplyr::mutate(freq = vulgar/words*1000) |>
  dplyr::filter(!is.na(freq)) |>
  dplyr::ungroup()  |>
  ggplot2::ggplot(aes(x = year_chart, y = freq, group = solo_gender, color = solo_gender, linetype = solo_gender)) +
  geom_line() +
  theme_bw() +
  theme(legend.position = "top",
        legend.title=element_blank()) +
  labs(y = "Relative frequency of vulgar elements\n(per 1,000 words)",
       x = "Year") +
  scale_color_manual(values = c("gray30", "gray70"))
# save
ggsave(here::here("images/Fig4.png"), width = 18, height = 10, units = "cm")
```

overview vulgar elements

```{r}
vtypes <- dat[, c(18, 44:148)] |>
  tidyr::gather(form, frequency, ass:whores) |>
  dplyr::group_by(form) |>
  dplyr::summarise(words = sum(no_words),
                   freq = sum(frequency)) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    lemma = case_when(
      str_detect(form, "^ass") ~ "ass(hole)",
      str_detect(form, "^ballsack$|^balls$") ~ "balls",
      str_detect(form, "^bastard") ~ "bastard",
      str_detect(form, "^biatch$|^bitch") ~ "bitch",
      str_detect(form, "^bimbo") ~ "bimbo",
      str_detect(form, "^boob") ~ "boobs",
      str_detect(form, "^booty|^booties") ~ "booty",
      str_detect(form, "^bullshit") ~ "bullshit",
      str_detect(form, "^bum|^butt") ~ "bum/butt",
      str_detect(form, "^clit") ~ "clit",
      str_detect(form, "^cock") ~ "cock",
      str_detect(form, "^coochie") ~ "coochie",
      str_detect(form, "^crap") ~ "crap",
      str_detect(form, "^cum") ~ "cum",
      str_detect(form, "^cunt") ~ "cunt",
      str_detect(form, "^damn|^dammit|^goddam") ~ "(god)damn",
      str_detect(form, "^dick") ~ "dick(head)",
      str_detect(form, "^doggystyle") ~ "doggystyle",
      str_detect(form, "^dork") ~ "dork",
      str_detect(form, "^dyke") ~ "dyke",
      str_detect(form, "^fag") ~ "fag/faggot",
      str_detect(form, "^fart") ~ "fart",
      str_detect(form, "^fuck|^fucka|^fucked|^fucker|^fuckin") ~ "fuck",
      str_detect(form, "^(mafuck|mothafuck|motherfuck)") ~ "(mother)fuck",
      str_detect(form, "^hell") ~ "hell",
      str_detect(form, "^ho$|^hoes$|^hos$") ~ "hoe",
      str_detect(form, "^homo") ~ "homo",
      str_detect(form, "^honkey") ~ "honkey",
      str_detect(form, "^hooker") ~ "hooker",
      str_detect(form, "^knob") ~ "knob",
      str_detect(form, "^negro|^nigg") ~ "n-word",
      str_detect(form, "^nutsack") ~ "(nut)sack",
      str_detect(form, "^nympho") ~ "nympho",
      str_detect(form, "^piss") ~ "piss",
      str_detect(form, "^poop") ~ "poop",
      str_detect(form, "^prick") ~ "prick",
      str_detect(form, "^pussy|^pussies") ~ "pussy",
      str_detect(form, "^retard") ~ "retard",
      str_detect(form, "^scat") ~ "scat",
      str_detect(form, "^shag") ~ "shag",
      str_detect(form, "^shit|^shitty|^shittin") ~ "shit",
      str_detect(form, "^skeet") ~ "skeet",
      str_detect(form, "^slut") ~ "slut",
      str_detect(form, "^thot") ~ "thot",
      str_detect(form, "^tits|^titt") ~ "tits",
      str_detect(form, "^whore") ~ "whore",
      TRUE ~ form )) |>
  dplyr::group_by(lemma) |>
  dplyr::summarise(frequency = sum(freq),
                   words = unique(words)) |>
  dplyr::arrange(-frequency) |>
  dplyr::ungroup() |>
  dplyr::mutate(lemma_redux = ifelse(frequency >= 10, lemma, "other")) |>
  dplyr::group_by(lemma_redux) |>
  dplyr::summarise(frequency = sum(frequency),
                   words = unique(words),
                   relfreq = round(frequency/words*1000, 2)) |>
  dplyr::arrange(-frequency) |>
  dplyr::ungroup() |>
  dplyr::mutate(totalvulgar = sum(frequency)) |>
  dplyr::group_by(lemma_redux) |>
  dplyr::mutate(frequency = 
                  paste0(frequency, " (", round(frequency/totalvulgar*100, 1), "%)", sep = ""))
# save
write.table(vtypes, here::here("tables/vtypes.txt"), sep = "\t", row.names = F)
# inspect
vtypes
```

visualize frequency of vulgar forms

```{r}
vtypes |>
  ggplot2::ggplot(aes(x = reorder(lemma_redux, relfreq), y = relfreq, label = frequency)) +
  geom_text(aes(label = lemma_redux), size = 3) +
  geom_text(aes(y = relfreq + 0.4), size = 3, color = "gray50") +
  coord_flip(ylim = c(0,2.75)) +
  theme_bw() +
  labs(y = "Relative frequency of lemma in corpus\n(per 1,000 words)",
       x = "Lemma")
# save
ggsave(here::here("images/Fig3.png"), width = 16, height = 16, units = "cm")
```

low frequency lemmas


```{r}
lowtypes <- dat[, c(18, 44:148)] |>
  tidyr::gather(form, frequency, ass:whores) |>
  dplyr::group_by(form) |>
  dplyr::summarise(words = sum(no_words),
                   freq = sum(frequency)) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    lemma = case_when(
      str_detect(form, "^ass") ~ "ass(hole)",
      str_detect(form, "^ballsack$|^balls$") ~ "balls",
      str_detect(form, "^bastard") ~ "bastard",
      str_detect(form, "^biatch$|^bitch") ~ "bitch",
      str_detect(form, "^bimbo") ~ "bimbo",
      str_detect(form, "^boob") ~ "boobs",
      str_detect(form, "^booty|^booties") ~ "booty",
      str_detect(form, "^bullshit") ~ "bullshit",
      str_detect(form, "^bum|^butt") ~ "bum/butt",
      str_detect(form, "^clit") ~ "clit",
      str_detect(form, "^cock") ~ "cock",
      str_detect(form, "^coochie") ~ "coochie",
      str_detect(form, "^crap") ~ "crap",
      str_detect(form, "^cum") ~ "cum",
      str_detect(form, "^cunt") ~ "cunt",
      str_detect(form, "^damn|^dammit|^goddam") ~ "(god)damn",
      str_detect(form, "^dick") ~ "dick(head)",
      str_detect(form, "^doggystyle") ~ "doggystyle",
      str_detect(form, "^dork") ~ "dork",
      str_detect(form, "^dyke") ~ "dyke",
      str_detect(form, "^fag") ~ "fag/faggot",
      str_detect(form, "^fart") ~ "fart",
      str_detect(form, "^fuck|^fucka|^fucked|^fucker|^fuckin") ~ "fuck",
      str_detect(form, "^(mafuck|mothafuck|motherfuck)") ~ "(mother)fuck",
      str_detect(form, "^hell") ~ "hell",
      str_detect(form, "^ho$|^hoes$|^hos$") ~ "hoe",
      str_detect(form, "^homo") ~ "homo",
      str_detect(form, "^honkey") ~ "honkey",
      str_detect(form, "^hooker") ~ "hooker",
      str_detect(form, "^knob") ~ "knob",
      str_detect(form, "^negro|^nigg") ~ "n-word",
      str_detect(form, "^nutsack") ~ "(nut)sack",
      str_detect(form, "^nympho") ~ "nympho",
      str_detect(form, "^piss") ~ "piss",
      str_detect(form, "^poop") ~ "poop",
      str_detect(form, "^prick") ~ "prick",
      str_detect(form, "^pussy|^pussies") ~ "pussy",
      str_detect(form, "^retard") ~ "retard",
      str_detect(form, "^scat") ~ "scat",
      str_detect(form, "^shag") ~ "shag",
      str_detect(form, "^shit|^shitty|^shittin") ~ "shit",
      str_detect(form, "^skeet") ~ "skeet",
      str_detect(form, "^slut") ~ "slut",
      str_detect(form, "^thot") ~ "thot",
      str_detect(form, "^tits|^titt") ~ "tits",
      str_detect(form, "^whore") ~ "whore",
      TRUE ~ form )) |>
  dplyr::group_by(lemma) |>
  dplyr::summarise(frequency = sum(freq),
                   words = unique(words)) |>
  dplyr::arrange(-frequency) |>
  dplyr::ungroup() |>
  dplyr::mutate(lemma_redux = ifelse(frequency < 10, lemma, "other")) |>
  dplyr::group_by(lemma_redux) |>
  dplyr::summarise(frequency = sum(frequency),
                   words = unique(words),
                   relfreq = round(frequency/words*1000, 2)) |>
  dplyr::arrange(-frequency)
# save
write.table(lowtypes, here::here("tables/lowtypes.txt"), sep = "\t", row.names = F)
# inspect
lowtypes
```


# Inferential Statistics

## CIT

perpare data

```{r}
# convert character strings to factors
citdat <- dat |>
  dplyr::mutate_if(is.character, factor) |>
  dplyr::mutate(contains_vulgarity = factor(contains_vulgarity))
```

perform CIT analysis 

```{r}
# set.seed
set.seed(utf8ToInt("RawCIT")) 

# create initial conditional inference tree model
citd.ctree <- partykit::ctree(contains_vulgarity ~ assigned_topic + year_chart + genre + artist_type + solo_gender + solo_nationality + performance_peak,
                              data = citdat)
plot(citd.ctree, gp = gpar(fontsize = 8)) # plot final ctree
```

re-code variables based on cit results

```{r}
citdat <- dat |>
  
  # re-categorising
  dplyr::mutate(genre = dplyr::case_when(genre == "hip-hop" ~ genre,
                                         T ~ "not hip-hop"))|>
  dplyr::mutate(assigned_topic = dplyr::case_when(
    assigned_topic == "Heartbreak" ~ "Nostalgia",
    assigned_topic == "Belonging" ~ "Nostalgia",
    assigned_topic == "Clubbing" ~ "Club",
    assigned_topic == "Street Culture" ~ "Gang",
    T ~ assigned_topic)) |>
  dplyr::mutate(artist_type = dplyr::case_when(
    artist_type == "Solo" ~ artist_type,
    T ~ "Group"))|>
  
  # renaming
  dplyr::rename(Vulgarity = contains_vulgarity,
                Topic = assigned_topic,
                Artist = artist_type,
                Year = year_chart,
                Genre = genre,
                Gender = solo_gender,
                Nationality = solo_nationality,
                TopPerformance = performance_peak) |>
  
  # reformatting
  dplyr::mutate_if(is.character, factor) |>
  dplyr::mutate(Vulgarity = factor(Vulgarity))
```

re-run CIT analysis with simplified variable levels

```{r}
# set.seed
set.seed(utf8ToInt("CIT")) 

# create initial conditional inference tree model
citd.ctree <- partykit::ctree(Vulgarity ~ Topic + Year + Genre + Artist + Gender + Nationality + TopPerformance,
                              data = citdat)
plot(citd.ctree, gp = gpar(fontsize = 8)) # plot final ctree
```

prettify CIT


```{r}
# extract p-values
pvals <- unlist(
  nodeapply(
    citd.ctree,
    ids = nodeids(citd.ctree),
    function(n) info_node(n)$p.value
  )
)
pvals <- pvals[pvals < .05]

# plotting
p <- ggparty(citd.ctree) +
  # --- draw edges first (background) ---
  geom_edge(alpha = 0.5, color = "gray40") +
  geom_edge_label(alpha = 0.9,
                  size = 3, 
                  fill = "white") +

  # --- draw node plots and labels on top ---
  geom_node_plot(
    gglist = list(
      geom_bar(
        aes(x = "", fill = Vulgarity),
        position = position_fill(),
        color = "black"
      ),
      theme_minimal(),
      
      scale_fill_manual(values = c("gray50", "gray80"), guide = FALSE),
      scale_y_continuous(breaks = c(0, 1)),
      xlab(""),
      ylab("Probability"),
      geom_text(
        aes(
          x = "",
          group = Vulgarity,
          label = stat(count)
        ),
        stat = "count",
        position = position_fill(),
        vjust = 1.05
      )
    ),
    shared_axis_labels = TRUE
  ) +

  # inner node labels (opaque background)
  geom_node_label(
    line_list = list(
      aes(label = splitvar),
      aes(
        label = paste0(
          "N=", nodesize, ", p",
          ifelse(
            pvals < .001,
            "<.001",
            paste0("=", round(pvals, 3))
          )
        ),
        size = 10
      )
    ),
    line_gpar = list(
      list(size = 10, fill = "white"),  # opaque background
      list(size = 10, fill = "white")
    ),
    ids = "inner"
  ) +

  # terminal node labels (opaque background)
  geom_node_label(
    aes(label = paste0("Node ", id, ", N = ", nodesize)),
    ids = "terminal",
    nudge_y = 0.05,
    nudge_x = 0.01,
    fill = "white",  # opaque background
    size = 3)
# show plot
p

# Save to file
ggsave(
  filename = here::here("images/CIT_VulgarityPop.png"),  # filename with extension
  plot = p,
  width = 30, height = 20, 
  dpi = 600,   # resolution
  units = "cm"
)
```

## Regression 

prepare data

```{r}
citdat |>
  dplyr::group_by(Year) |>
  dplyr::summarise(words = sum(no_words),
                   vulgar = sum(total_bad_word_count)) |>
  dplyr::mutate(freq = vulgar/words*1000) |>
  dplyr::filter(!is.na(freq)) |>
  dplyr::ungroup()  |>
  dplyr::mutate(Year = Year-2000) -> m1dat
# inspect
head(m1dat)
```


run simple linear model

```{r}
# linear regression
m1 <- lm(freq ~ Year, data = m1dat)
# inspect
sjPlot::tab_model(m1)
```

report linear model

```{r}
report::report(m1)
```

## GLMM

What factors impact the use of vulgarity?

inspect data

```{r}
citdat |>
  dplyr::filter(Gender != "NA") |>
  dplyr::mutate(Vulgarity = as.numeric(Vulgarity)) |>
  
  ggplot(aes(Topic, Vulgarity, color = Gender)) +
  facet_wrap(~Nationality) +
  stat_summary(fun = mean, geom = "point") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  theme_set(theme_bw(base_size = 10)) +
  theme(legend.position = "top") +
  labs(x = "", y = "Observed Probabilty of vulgarity") +
  scale_color_manual(values = gray.colors(3, start = 0.1, end = 0.7)) 
```

set options

```{r}
# set options
options(contrasts  =c("contr.treatment", "contr.poly"))
dat.dist <- datadist(citdat)
options(datadist = "dat.dist")
```


generate baseline model

```{r}
# baseline model glm
m0.glm = glm(Vulgarity ~ 1, family = binomial, data = citdat) 
# base-line mixed-model
m0.glmer = glmer(Vulgarity ~ (1|Genre), data = citdat, family = binomial) 
```

check random intercepts

```{r}
aic.glmer <- AIC(logLik(m0.glmer))
aic.glm <- AIC(logLik(m0.glm))
aic.glmer; aic.glm
```

test random intercepts

```{r}
# test random effects
null.id = -2 * logLik(m0.glm) + 2 * logLik(m0.glmer)
pchisq(as.numeric(null.id), df=1, lower.tail=F) 
```

automated model fitting (not run - takes very long: 18+ hours)

```{r, eval = F}
# wrapper function for linear mixed-models
glmer.glmulti <- function(formula,data, random="",...){
  glmer(paste(deparse(formula),random), 
        family = binomial, 
        data=data, 
        control=glmerControl(optimizer="bobyqa"), ...)
}
# define formula
form_glmulti = as.formula(paste("Vulgarity ~ Gender + Year + Artist + Nationality + Topic + TopPerformance"))
# multi selection for glmer
mfit <- glmulti::glmulti(form_glmulti,random="+(1 | Genre)", 
                data = citdat, method = "h", fitfunc = glmer.glmulti,
                crit = "bic", intercept = TRUE, marginality = FALSE, level = 2)
```

extract best models (not shown as automated fitting is not run here again)

```{r, eval = F}
# extract best models
top <- weightable(mfit)
top <- top[1:5,]
# inspect top 5 models
top
```

Best models:

After 2100 models:
Best model: Vulgarity~1+Artist+Year+Nationality:Year+Topic:Year
Crit= 2493.43348486754
Mean crit= 2584.32378740494

After 10250 models:
Best model: Vulgarity~1+Artist+Year+Nationality:Year+Topic:Year
Crit= 2493.43348486754
Mean crit= 2562.69269557276

After 11450 models:
Best model: Vulgarity~1+Artist+Year+Nationality:Year+Topic:Year
Crit= 2493.43348486754
Mean crit= 2506.13180345995

inspect best model

```{r}
rdat <- citdat |>
  dplyr::select(Vulgarity, Genre, Artist, Year, Nationality, Topic)
```



```{r, warning=F, message=F}
mlr.glmer <- glmer(Vulgarity ~ (1 | Genre) + Artist + Year + Nationality + Topic, 
                   family = binomial,
                   control=glmerControl(optimizer="bobyqa"),
                   data = rdat)
# inspect final minimal adequate model
summary(mlr.glmer, corr = F)
```

re-create null-model

```{r}
# baseline model glm
m0.glm = glm(Vulgarity ~ 1, family = binomial, data = rdat) 
# base-line mixed-model
m0.glmer = glmer(Vulgarity ~ (1|Genre), data = rdat, family = binomial) 
```


check performance
 
```{r}
# final model better than base-line model
sigfit <- anova(mlr.glmer, m0.glmer, test = "Chi") 
# inspect
sigfit
```
 

```{r}
performance::performance(mlr.glmer)
```

extract fit statistics

```{r}
probs = 1/(1+exp(-fitted(mlr.glmer)))
probs = binomial()$linkinv(fitted(mlr.glmer))
somers2(probs, as.numeric(citdat$Vulgarity)-1)
```

```{r}
diagnostic_plots <- plot(check_model(mlr.glmer, panel = FALSE))
p1 <- diagnostic_plots[1]
p1
```

```{r}
p2 <- diagnostic_plots[2]
p2
```

```{r}
p3 <- diagnostic_plots[3]
p3
```

```{r}
p4 <- diagnostic_plots[4]
p4
```

```{r}
p5 <- diagnostic_plots[5]
p5
```


```{r}
p6 <- diagnostic_plots[6]
p6
```


 visualise effects
 
 generate dummy data to plot effects


```{r}
# Define factor levels
vulgarity <- names(table(rdat$Vulgarity))
artist <- names(table(rdat$Artist))
topic <- names(table(rdat$Topic))
year <- names(table(rdat$Year))
nationality <- names(table(rdat$Nationality))
genre <- names(table(rdat$Genre))

# Create full factorial dataset
df <- expand.grid(
  Vulgarity = vulgarity,
  Artist = artist,
  Topic = topic,
  Year = year,
  Nationality = nationality,
  Genre = genre
)

# Convert to factors (optional, but recommended)
df$Vulgarity <- factor(df$Vulgarity, levels = c(0, 1))
df$Artist <- factor(df$Artist)
df$Topic <- factor(df$Topic)
df$Year <- as.numeric(as.character(df$Year))
df$Nationality <- factor(df$Nationality)
df$Genre <- factor(df$Genre)

# inspect
head(df)
```

add predicitons to data

```{r}
df$Prediction <- predict(mlr.glmer, newdata = df, type="response")
df$Prediction <- df$Prediction
# inspect
head(df)
```

visualise effects

```{r}
df |>
  dplyr::filter(Vulgarity == 1)  |>
  ggplot(aes(y = Prediction, x = Year, color = Nationality)) +
  facet_grid(Topic~Genre) +
  stat_summary(fun = mean, geom = "point") +
  #stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  theme_set(theme_bw(base_size = 10)) +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 90)) +
  labs(x = "", y = "Predicted Probabilty of Vulgarity") +
  coord_cartesian(ylim = c(0, 1)) +
  scale_color_manual(values = gray.colors(3, start = 0.1, end = 0.7)) 
# save
ggsave(here::here("images/Fig7.png"), width = 18, height = 12, units = "cm")
```

```{r}
df |>
  dplyr::filter(Vulgarity == 1)  |>
  ggplot(aes(y = Prediction, x = reorder(Topic, -Prediction))) +
  stat_summary(fun = mean, geom = "point") +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  theme_set(theme_bw(base_size = 10)) +
  theme(legend.position = "top",
        axis.text.x = element_text(angle = 90)) +
  labs(x = "", y = "Predicted Probabilty of Vulgarity") +
  coord_cartesian(ylim = c(0, 1)) +
  scale_color_manual(values = gray.colors(3, start = 0.1, end = 0.7)) 
# save
ggsave(here::here("images/Fig8.png"), width = 18, height = 12, units = "cm")
```

summarize final model

```{r}
sjPlot::tab_model(mlr.glmer)
```

report model

```{r}
report::report(mlr.glmer)
```


# Outro

```{r}
sessionInfo()
```

